"""
DAG de Pipeline Big Data Complet
================================
Ce DAG orchestre le pipeline complet :
1. Collecte des donnÃ©es (scraping)
2. Stockage en CSV
3. Nettoyage et prÃ©traitement NLP

"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
from datetime import timedelta
import sys
import os

# Configuration des chemins
PREPROCESSING_PATH = '/opt/airflow/preprocessing'
INPUT_FILE = '/opt/airflow/preprocessing/ai_vs_human_news.csv'
OUTPUT_FILE = '/opt/airflow/preprocessing/processed_news.csv'

sys.path.insert(0, PREPROCESSING_PATH)


# ===================== TÃ‚CHE 1: COLLECTE DES DONNÃ‰ES =====================
def collect_data(**kwargs):
    """
    Simule la collecte de nouvelles donnÃ©es.
    """
    from datetime import datetime
    
    print("ðŸ“¡ Collecte des donnÃ©es en cours...")
    
    # Simulation de donnÃ©es collectÃ©es
    data = {
        'source': 'newsapi',
        'title': f'Article collectÃ© le {datetime.now().strftime("%Y-%m-%d %H:%M")}',
        'content': 'This is sample article content with URLs https://example.com and emojis ðŸ˜Š The CATS are running!!!',
        'published_at': datetime.now().isoformat()
    }
    
    print(f"âœ… DonnÃ©es collectÃ©es: {data['title']}")
    return data


# ===================== TÃ‚CHE 2: STOCKAGE EN CSV =====================
def store_to_csv(**kwargs):
    """
    Stocke les donnÃ©es dans un fichier CSV.
    """
    import pandas as pd
    
    ti = kwargs['ti']
    new_data = ti.xcom_pull(task_ids='collect_data')
    
    print("ðŸ’¾ Stockage des donnÃ©es...")
    
    # Utiliser le fichier existant
    csv_path = '/opt/airflow/preprocessing/ai_vs_human_news.csv'
    
    # Lire le fichier existant
    try:
        df = pd.read_csv(csv_path)
        print(f"ðŸ“Š Fichier existant chargÃ©: {len(df)} lignes")
    except:
        df = pd.DataFrame()
        print("ðŸ“„ CrÃ©ation d'un nouveau fichier")
    
    print(f"âœ… DonnÃ©es prÃªtes pour le prÃ©traitement")
    return csv_path


# ===================== TÃ‚CHE 3: PRÃ‰TRAITEMENT =====================
def run_preprocessing(**kwargs):
    """
    ExÃ©cute le pipeline de nettoyage.
    """
    import pandas as pd
    sys.path.insert(0, '/opt/airflow/preprocessing')
    
    from cleaner import clean_text
    from normalizer import normalize_text
    from nlp_processor import process_nlp
    
    ti = kwargs['ti']
    input_file = ti.xcom_pull(task_ids='store_to_csv')
    output_file = '/opt/airflow/preprocessing/processed_news.csv'
    
    print("ðŸ§¹ PrÃ©traitement en cours...")
    
    df = pd.read_csv(input_file)
    
    # Trouver la colonne de texte
    text_col = None
    for col in ['content', 'article', 'text', 'News']:
        if col in df.columns:
            text_col = col
            break
    
    if text_col is None:
        text_col = df.columns[0]
    
    print(f"ðŸ“ Traitement de la colonne: {text_col}")
    
    def preprocess(text):
        if not isinstance(text, str):
            return []
        cleaned = clean_text(text)
        normalized = normalize_text(cleaned)
        tokens = process_nlp(normalized)
        return tokens
    
    df['processed_tokens'] = df[text_col].apply(preprocess)
    df.to_csv(output_file, index=False)
    
    print(f"âœ… PrÃ©traitement terminÃ©! {len(df)} lignes traitÃ©es")
    print(f"ðŸ“ Fichier sauvegardÃ©: {output_file}")


# ===================== CONFIGURATION DU DAG =====================
default_args = {
    'owner': 'data_engineering',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(seconds=30),
}

with DAG(
    dag_id='bigdata_pipeline',
    default_args=default_args,
    description='Pipeline Big Data: Collecte â†’ Stockage â†’ PrÃ©traitement',
    schedule_interval=timedelta(days=1),
    start_date=days_ago(1),
    catchup=False,
    tags=['bigdata', 'scraping', 'preprocessing', 'nlp'],
) as dag:
    
    # TÃ¢che 1: Collecte
    t1_collect = PythonOperator(
        task_id='collect_data',
        python_callable=collect_data,
    )
    
    # TÃ¢che 2: Stockage
    t2_store = PythonOperator(
        task_id='store_to_csv',
        python_callable=store_to_csv,
    )
    
    # TÃ¢che 3: PrÃ©traitement
    t3_preprocess = PythonOperator(
        task_id='run_preprocessing',
        python_callable=run_preprocessing,
    )
    
    # Ordre d'exÃ©cution
    t1_collect >> t2_store >> t3_preprocess
